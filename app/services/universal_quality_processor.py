import json
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple
from enum import Enum
from quixstreams import Application
from quixstreams.models import TopicConfig

from app.core.config import get_settings
from app.cv.metrics_storage import MetricsStorage
from app.utils.logger import setup_logger


class UniversalQualityProcessor:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –ª—é–±—ã—Ö —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–π
    
    –†–∞–±–æ—Ç–∞–µ—Ç —Å –Ω–æ–≤–æ–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π v2.0
    –†–∞—Å—à–∏—Ä—è–µ–º–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: –æ–¥–∏–Ω –∞–ª–≥–æ—Ä–∏—Ç–º ‚Üí –ª—é–±—ã–µ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è
    """
    
    def __init__(self):
        self.settings = get_settings()
        self.logger = setup_logger("universal_quality_processor")
        self.metrics_storage = MetricsStorage(self.settings.metrics_dir)
        
        # Quix Streams Application
        self.app = Application(
            broker_address=self.settings.kafka_bootstrap_servers,
            consumer_group=f"{self.settings.kafka_group_id}_universal",
            auto_offset_reset="earliest",
        )
        
        # –¢–æ–ø–∏–∫–∏
        self.input_topic = self.app.topic(
            self.settings.kafka_input_topic,
            config=TopicConfig(num_partitions=3, replication_factor=1)
        )
        self.output_topic = self.app.topic(
            "performance-feedback",
            config=TopicConfig(num_partitions=3, replication_factor=1)
        )

    def create_universal_pipeline(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ pipeline –¥–ª—è –ª—é–±–æ–≥–æ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è"""
        self.logger.info("üöÄ Creating universal exercise quality pipeline...")
        
        sdf = self.app.dataframe(self.input_topic)
        
        # –ë–∞–∑–æ–≤—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
        sdf = sdf.apply(self._parse_cv_result)
        sdf = sdf.filter(lambda frame: frame is not None)
        sdf = sdf.group_by("session_uuid")
        
        # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π FSM –∞–Ω–∞–ª–∏–∑
        sdf = sdf.apply(self._analyze_exercise_universal, stateful=True)
        
        # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –∫–æ–¥–æ–≤
        sdf = sdf.apply(self._generate_semantic_error_codes)
        
        # –í—Ä–µ–º–µ–Ω–Ω–∞—è –∞–≥—Ä–µ–≥–∞—Ü–∏—è (3 —Å–µ–∫—É–Ω–¥—ã)
        sdf = sdf.tumbling_window(duration_ms=3000, grace_ms=500)
        sdf = sdf.reduce(self._aggregate_semantic_patterns, self._init_semantic_state)
        
        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è LLM
        sdf = sdf.final()
        sdf = sdf.apply(self._analyze_semantic_coaching, stateful=True)
        
        # –§–∏–ª—å—Ç—Ä –∑–Ω–∞—á–∏–º—ã—Ö coaching signals
        sdf = sdf.filter(lambda result: result.get("requires_coaching", False))
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è LLM
        sdf = sdf.apply(self._prepare_llm_semantic_input)
        
        sdf = sdf.to_topic(self.output_topic)
        
        return sdf

    def _parse_cv_result(self, message) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –æ—Ç CV —Å–µ—Ä–≤–∏—Å–∞"""
        try:
            # Handle both bytes and dict inputs
            if isinstance(message, bytes):
                data = json.loads(message.decode("utf-8"))
            elif isinstance(message, dict):
                data = message
            else:
                # Try to parse as string
                data = json.loads(str(message))
            
            self.logger.debug(f"üì• Universal parser - frame: {data.get('frame_track_uuid', 'unknown')}")
            return data
        except Exception as e:
            self.logger.error(f"‚ùå Parse error: {e}, message type: {type(message)}")
            return None

    def _analyze_exercise_universal(self, frame: Dict[str, Any], state: Dict[str, Any]) -> Dict[str, Any]:
        """
        –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è —Å –∑–∞–≥—Ä—É–∑–∫–æ–π —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        """
        try:
            session_uuid = frame['session_uuid']
            exercise_name = frame.get('exercise')
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏
            if not state.exists(session_uuid):
                state.set(session_uuid, {
                    'exercise_config': None,
                    'fsm_context': {
                        'current_state': None,
                        'prev_state': None,
                        'state_start_time': frame['current_time'],
                        'state_duration': 0.0,
                        'transition_history': [],
                        'exercise_phase': 'unknown'
                    },
                    'level': 'beginner'  # TODO: –ø–æ–ª—É—á–∞—Ç—å –∏–∑ user profile
                })
            
            session_state = state.get(session_uuid)
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è (v2.0)
            if session_state.get('exercise_config') is None:
                try:
                    # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–æ–≤—É—é –≤–µ—Ä—Å–∏—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
                    config_path = f"{exercise_name}-v2.json"
                    session_state['exercise_config'] = self.metrics_storage.get_metrics_for_exercise_v2(config_path)
                    self.logger.info(f"‚úÖ Loaded v2.0 config for {exercise_name}")
                except:
                    # Fallback –Ω–∞ —Å—Ç–∞—Ä—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –µ—Å–ª–∏ v2 –Ω–µ –Ω–∞–π–¥–µ–Ω–∞
                    self.logger.warning(f"‚ö†Ô∏è v2.0 config not found for {exercise_name}, using legacy")
                    session_state['exercise_config'] = self.metrics_storage.get_metrics_for_exercise(exercise_name)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
                state.set(session_uuid, session_state)
            
            exercise_config = session_state['exercise_config']
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —É–≥–ª–æ–≤
            angles = {
                angle['name']: angle['value']
                for angle in frame['metrics']['angles']
            }
            
            # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π FSM –∞–Ω–∞–ª–∏–∑
            fsm_context = self._analyze_fsm_universal(angles, exercise_config, session_state, frame['current_time'])
            session_state['fsm_context'] = fsm_context
            state.set(session_uuid, session_state)
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∫ –∫–∞–¥—Ä—É
            frame['exercise_context'] = {
                'exercise_name': exercise_name,
                'config_version': exercise_config.get('version', '1.0'),
                'fsm_context': fsm_context,
                'angles': angles,
                'level': session_state['level']
            }
            
            self.logger.debug(f"üéØ Universal FSM - {exercise_name}: {fsm_context['current_state']} "
                            f"({fsm_context['exercise_phase']}) duration: {fsm_context['state_duration']:.1f}s")
            
            return frame
            
        except Exception as e:
            self.logger.error(f"‚ùå Universal analysis error: {e}")
            frame['exercise_context'] = {'error': str(e)}
            return frame

    def _analyze_fsm_universal(self, angles: Dict[str, float], exercise_config: Dict, 
                             session_state: Dict, current_time: float) -> Dict[str, Any]:
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ FSM –¥–ª—è –ª—é–±–æ–≥–æ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è"""
        
        fsm_context = session_state['fsm_context']
        level = session_state['level']
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ FSM –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        if 'finite_state_machine' in exercise_config:
            # –ù–æ–≤–∞—è v2.0 —Å—Ç—Ä—É–∫—Ç—É—Ä–∞
            fsm_config = exercise_config['finite_state_machine'][level]
        else:
            # –°—Ç–∞—Ä–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞
            fsm_config = exercise_config['state_machine'][level]
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        current_state = self._determine_state_universal(angles, fsm_config['states'])
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ FSM –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        prev_state = fsm_context['current_state']
        
        if current_state != prev_state:
            # –ü–µ—Ä–µ—Ö–æ–¥ –≤ –Ω–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            if prev_state is not None:
                transition = {
                    'from': prev_state,
                    'to': current_state,
                    'timestamp': current_time,
                    'duration_in_prev': fsm_context['state_duration']
                }
                fsm_context['transition_history'].append(transition)
                
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é
                if len(fsm_context['transition_history']) > 10:
                    fsm_context['transition_history'] = fsm_context['transition_history'][-10:]
            
            fsm_context['prev_state'] = prev_state
            fsm_context['current_state'] = current_state
            fsm_context['state_start_time'] = current_time
            fsm_context['state_duration'] = 0.0
        else:
            # –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –≤ —Ç–µ–∫—É—â–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏
            fsm_context['state_duration'] = current_time - fsm_context['state_start_time']
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–∞–∑—ã —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è
        fsm_context['exercise_phase'] = self._determine_exercise_phase_universal(
            current_state, fsm_config['states']
        )
        
        return fsm_context

    def _determine_state_universal(self, angles: Dict[str, float], states: List[Dict]) -> str:
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è FSM"""
        for state in states:
            condition = state["condition"]
            try:
                # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —É—Å–ª–æ–≤–∏–π
                if self._evaluate_condition_safe(condition, angles):
                    return state["id"]
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è State evaluation error for '{condition}': {e}")
                continue
        
        return "s1"  # –î–µ—Ñ–æ–ª—Ç–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ

    def _evaluate_condition_safe(self, condition: str, angles: Dict[str, float]) -> bool:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —É—Å–ª–æ–≤–∏–π –±–µ–∑ eval()"""
        try:
            # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è eval
            safe_dict = {k: v for k, v in angles.items() if isinstance(v, (int, float))}
            return eval(condition, {"__builtins__": {}}, safe_dict)
        except:
            return False

    def _determine_exercise_phase_universal(self, current_state: str, states: List[Dict]) -> str:
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–∞–∑—ã —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è"""
        for state in states:
            if state['id'] == current_state:
                return state.get('exercise_phase', 'unknown')
        return 'unknown'

    def _generate_semantic_error_codes(self, frame: Dict[str, Any]) -> Dict[str, Any]:
        """
        –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –∫–æ–¥–æ–≤ –æ—à–∏–±–æ–∫ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        """
        context = frame.get('exercise_context', {})
        if 'error' in context:
            return frame
        
        error_codes = []
        
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            exercise_name = context.get('exercise_name')
            config_version = context.get('config_version', '1.0')
            
            if config_version == '2.0':
                # –ù–æ–≤–∞—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
                error_codes = self._process_semantic_taxonomy(frame, context)
            else:
                # Fallback –Ω–∞ —Å—Ç–∞—Ä—É—é —Å–∏—Å—Ç–µ–º—É
                error_codes = self._process_legacy_thresholds(frame, context)
            
            frame['semantic_error_codes'] = error_codes
            
            self.logger.debug(f"üîç Generated {len(error_codes)} semantic codes for {exercise_name}")
            
        except Exception as e:
            self.logger.error(f"‚ùå Semantic code generation error: {e}")
            frame['semantic_error_codes'] = []
        
        return frame

    def _process_semantic_taxonomy(self, frame: Dict[str, Any], context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Ç–∞–∫—Å–æ–Ω–æ–º–∏–∏ v2.0"""
        error_codes = []
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        exercise_config = self._load_exercise_config_from_context(context)
        if not exercise_config or 'error_taxonomy' not in exercise_config:
            return error_codes
        
        taxonomy = exercise_config['error_taxonomy']
        angles = context.get('angles', {})
        fsm_context = context.get('fsm_context', {})
        current_state = fsm_context.get('current_state', 's1')
        state_duration = fsm_context.get('state_duration', 0.0)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –æ—à–∏–±–æ–∫
        for category, violations in taxonomy.items():
            for violation_type, config in violations.items():
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç–∏ –∫ —Ç–µ–∫—É—â–µ–º—É —Å–æ—Å—Ç–æ—è–Ω–∏—é
                applicable_states = config.get('states_applicable', [])
                if applicable_states != ['all'] and current_state not in applicable_states:
                    continue
                
                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞ –æ—à–∏–±–∫–∏
                error_code = self._check_violation_universal(
                    config, angles, current_state, state_duration, category, violation_type
                )
                
                if error_code:
                    error_codes.append(error_code)
        
        return error_codes

    def _check_violation_universal(self, violation_config: Dict, angles: Dict[str, float], 
                                 current_state: str, state_duration: float,
                                 category: str, violation_type: str) -> Optional[Dict[str, Any]]:
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞—Ä—É—à–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        
        detection = violation_config.get('detection', {})
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —É–≥–ª–∞
        if 'angle' in detection:
            angle_name = detection['angle']
            if angle_name not in angles:
                return None
            
            angle_value = angles[angle_name]
            
            # State-specific detection
            if 'state_specific_detection' in violation_config:
                state_config = violation_config['state_specific_detection'].get(current_state)
                if state_config:
                    return self._check_angle_violation(
                        angle_value, state_config, violation_config, current_state, category, violation_type
                    )
            
            # State-specific thresholds
            elif 'state_specific_thresholds' in detection:
                state_thresholds = detection['state_specific_thresholds'].get(current_state)
                if state_thresholds:
                    return self._check_threshold_violation(
                        angle_value, state_thresholds, violation_config, current_state, category, violation_type
                    )
            
            # Simple severity levels
            elif 'severity_levels' in detection:
                return self._check_severity_levels(
                    angle_value, detection['severity_levels'], violation_config, current_state, category, violation_type
                )
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ä–µ–º–µ–Ω–∏
        elif detection.get('method') == 'temporal_analysis':
            max_duration = detection.get('max_duration_per_state', {}).get(current_state)
            if max_duration and state_duration > max_duration:
                return self._create_semantic_error_code(
                    violation_config, current_state, category, violation_type,
                    {'duration': state_duration, 'max_allowed': max_duration}
                )
        
        return None

    def _check_angle_violation(self, angle_value: float, state_config: Dict, 
                             violation_config: Dict, current_state: str, 
                             category: str, violation_type: str) -> Optional[Dict[str, Any]]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞—Ä—É—à–µ–Ω–∏—è —É–≥–ª–∞ —Å state-specific –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π"""
        
        optimal_range = state_config.get('optimal_range', [])
        if len(optimal_range) != 2:
            return None
        
        min_optimal, max_optimal = optimal_range
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞—Ä—É—à–µ–Ω–∏—è –¥–∏–∞–ø–∞–∑–æ–Ω–∞
        if min_optimal <= angle_value <= max_optimal:
            return None  # –í –Ω–æ—Ä–º–µ
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏
        severity_levels = state_config.get('severity_levels', {})
        
        for severity, level_config in severity_levels.items():
            level_range = level_config.get('range', [])
            if len(level_range) == 2:
                min_range, max_range = level_range
                if min_range <= angle_value <= max_range:
                    # –ù–∞—Ä—É—à–µ–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–æ
                    code_suffix = level_config.get('code_suffix', severity.upper())
                    return self._create_semantic_error_code(
                        violation_config, current_state, category, violation_type,
                        {
                            'angle_value': angle_value,
                            'optimal_range': optimal_range,
                            'severity': severity,
                            'code_suffix': code_suffix
                        }
                    )
        
        return None

    def _check_threshold_violation(self, angle_value: float, thresholds: Dict,
                                 violation_config: Dict, current_state: str,
                                 category: str, violation_type: str) -> Optional[Dict[str, Any]]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞—Ä—É—à–µ–Ω–∏—è –ø–æ—Ä–æ–≥–æ–≤"""
        
        optimal_range = thresholds.get('optimal_range', [])
        if len(optimal_range) != 2:
            return None
        
        min_optimal, max_optimal = optimal_range
        
        if min_optimal <= angle_value <= max_optimal:
            return None
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ severity levels
        severity_levels = thresholds.get('severity_levels', {})
        
        for severity, level_config in severity_levels.items():
            level_range = level_config.get('range', [])
            if len(level_range) == 2:
                min_range, max_range = level_range
                if min_range <= angle_value <= max_range:
                    return self._create_semantic_error_code(
                        violation_config, current_state, category, violation_type,
                        {
                            'angle_value': angle_value,
                            'optimal_range': optimal_range,
                            'severity': severity
                        }
                    )
        
        return None

    def _check_severity_levels(self, angle_value: float, severity_levels: Dict,
                             violation_config: Dict, current_state: str,
                             category: str, violation_type: str) -> Optional[Dict[str, Any]]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ—Å—Ç—ã—Ö —É—Ä–æ–≤–Ω–µ–π —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏"""
        
        detection = violation_config.get('detection', {})
        threshold_type = detection.get('threshold_type', 'absolute_deviation')
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ range_safety —Ç–∏–ø–∞
        if threshold_type == 'range_safety':
            safe_range = detection.get('safe_range', [])
            if len(safe_range) == 2:
                min_safe, max_safe = safe_range
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ —É–≥–æ–ª –≤–Ω–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞
                if angle_value < min_safe or angle_value > max_safe:
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–æ–≥–æ, –Ω–∞—Å–∫–æ–ª—å–∫–æ –¥–∞–ª–µ–∫–æ –æ—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞
                    for severity, level_config in severity_levels.items():
                        level_range = level_config.get('range', [])
                        if len(level_range) == 2:
                            min_range, max_range = level_range
                            if min_range <= angle_value <= max_range:
                                return self._create_semantic_error_code(
                                    violation_config, current_state, category, violation_type,
                                    {
                                        'angle_value': angle_value,
                                        'safe_range': safe_range,
                                        'violation_range': level_range,
                                        'severity': severity
                                    }
                                )
            return None
        
        # –°—É—â–µ—Å—Ç–≤—É—é—â–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è –¥—Ä—É–≥–∏—Ö —Ç–∏–ø–æ–≤ –ø–æ—Ä–æ–≥–æ–≤
        for severity, level_config in severity_levels.items():
            threshold = level_config.get('threshold')
            if threshold is None:
                continue
            
            if threshold_type == 'absolute_deviation' and abs(angle_value) > threshold:
                return self._create_semantic_error_code(
                    violation_config, current_state, category, violation_type,
                    {
                        'angle_value': angle_value,
                        'threshold': threshold,
                        'severity': severity
                    }
                )
            elif threshold_type == 'deviation_from_target':
                target_angle = detection.get('target_angle', 0)
                deviation = abs(angle_value - target_angle)
                if deviation > threshold:
                    return self._create_semantic_error_code(
                        violation_config, current_state, category, violation_type,
                        {
                            'angle_value': angle_value,
                            'target_angle': target_angle,
                            'deviation': deviation,
                            'threshold': threshold,
                            'severity': severity
                        }
                    )
        
        return None

    def _create_semantic_error_code(self, violation_config: Dict, current_state: str,
                                  category: str, violation_type: str, 
                                  metrics: Dict[str, Any]) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–¥–∞ –æ—à–∏–±–∫–∏"""
        
        code_prefix = violation_config.get('code_prefix', f"{category.upper()}_{violation_type.upper()}")
        code_suffix = metrics.get('code_suffix', current_state.upper())
        
        code = f"{code_prefix}_{code_suffix}"
        
        return {
            'code': code,
            'timestamp': datetime.now().isoformat(),
            'category': category,
            'violation_type': violation_type,
            'description': violation_config.get('description', ''),
            'coaching_priority': violation_config.get('coaching_priority', 'medium'),
            'semantic_category': violation_config.get('semantic_category', 'unknown'),
            'immediate_intervention': violation_config.get('immediate_intervention', False),
            'context': {
                'exercise_state': current_state,
                'metrics': metrics
            }
        }

    def _process_legacy_thresholds(self, frame: Dict[str, Any], context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Fallback –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —Å—Ç–∞—Ä—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π"""
        # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å–æ —Å—Ç–∞—Ä—ã–º–∏ —Ñ–æ—Ä–º–∞—Ç–∞–º–∏
        self.logger.info("üì± Processing legacy threshold format")
        return []

    def _load_exercise_config_from_context(self, context: Dict[str, Any]) -> Optional[Dict]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        exercise_name = context.get('exercise_name')
        if not exercise_name:
            return None
        
        try:
            # –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ v2.0 –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            config_path = f"{exercise_name}-v2.json"
            return self.metrics_storage.get_metrics_for_exercise_v2(config_path)
        except:
            return None

    def _init_semantic_state(self, value: Any) -> Dict[str, Any]:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∞–≥—Ä–µ–≥–∞—Ü–∏–∏"""
        return {
            'window_start': None,
            'window_end': None,
            'session_uuid': None,
            'exercise': None,
            'frames_processed': 0,
            'semantic_patterns': {},  # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –æ—à–∏–±–æ–∫
            'fsm_flow': [],  # –ü–æ—Ç–æ–∫ FSM –ø–µ—Ä–µ—Ö–æ–¥–æ–≤
            'coaching_categories': {
                'safety_concerns': [],
                'technical_issues': [],
                'form_deviations': [],
                'execution_problems': []
            }
        }

    def _aggregate_semantic_patterns(self, window_state: Dict[str, Any], 
                                   frame: Dict[str, Any]) -> Dict[str, Any]:
        """
        –ê–≥—Ä–µ–≥–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤ –≤—Ä–µ–º–µ–Ω–Ω–æ–º –æ–∫–Ω–µ
        """
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        if window_state['session_uuid'] is None:
            window_state['session_uuid'] = frame['session_uuid']
            window_state['exercise'] = frame['exercise']
            window_state['window_start'] = frame['current_time']
        
        window_state['window_end'] = frame['current_time']
        window_state['frames_processed'] += 1
        
        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è FSM –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        exercise_context = frame.get('exercise_context', {})
        fsm_context = exercise_context.get('fsm_context', {})
        
        if fsm_context.get('current_state'):
            transition_history = fsm_context.get('transition_history', [])
            for transition in transition_history:
                if transition not in window_state['fsm_flow']:
                    window_state['fsm_flow'].append(transition)
        
        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –∫–æ–¥–æ–≤
        semantic_codes = frame.get('semantic_error_codes', [])
        
        for error_code in semantic_codes:
            code = error_code['code']
            category = error_code['category']
            semantic_category = error_code.get('semantic_category', 'unknown')
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
            if code not in window_state['semantic_patterns']:
                window_state['semantic_patterns'][code] = {
                    'count': 0,
                    'first_seen': error_code['timestamp'],
                    'category': category,
                    'semantic_category': semantic_category,
                    'coaching_priority': error_code['coaching_priority'],
                    'description': error_code['description'],
                    'contexts': []
                }
            
            pattern = window_state['semantic_patterns'][code]
            pattern['count'] += 1
            pattern['last_seen'] = error_code['timestamp']
            pattern['contexts'].append(error_code['context'])
            
            # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º (—Ç–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–æ–¥—ã)
            if semantic_category == 'safety_concern':
                if code not in window_state['coaching_categories']['safety_concerns']:
                    window_state['coaching_categories']['safety_concerns'].append(code)
            elif semantic_category == 'technical_issue':
                if code not in window_state['coaching_categories']['technical_issues']:
                    window_state['coaching_categories']['technical_issues'].append(code)
            elif semantic_category == 'form_deviation':
                if code not in window_state['coaching_categories']['form_deviations']:
                    window_state['coaching_categories']['form_deviations'].append(code)
            else:
                if code not in window_state['coaching_categories']['execution_problems']:
                    window_state['coaching_categories']['execution_problems'].append(code)
        
        self.logger.debug(f"üìä Semantic aggregation - patterns: {len(window_state['semantic_patterns'])}")
        
        return window_state

    def _analyze_semantic_coaching(self, window_result: Dict[str, Any], 
                                 state: Dict[str, Any]) -> Dict[str, Any]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è coaching"""
        
        # Debug window structure
        self.logger.debug(f"üîç Window result keys: {list(window_result.keys())}")
        
        # Extract data from window result value
        window_data = window_result.get('value', window_result)
        session_uuid = window_data.get('session_uuid')
        if not session_uuid:
            self.logger.error(f"‚ùå No session_uuid in window data: {window_data}")
            return window_result
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏–∏ —Å–µ—Å—Å–∏–∏
        if not state.exists(session_uuid):
            state.set(session_uuid, {
                'windows_processed': 0,
                'semantic_history': {},
                'coaching_cadence': {
                    'last_coaching_sent': 0,
                    'coaching_count': 0
                }
            })
        
        session_state = state.get(session_uuid)
        session_state['windows_processed'] += 1
        state.set(session_uuid, session_state)
        
        # –ê–Ω–∞–ª–∏–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        requires_coaching = self._should_send_semantic_coaching(window_data, session_state)
        
        if requires_coaching:
            session_state['coaching_cadence']['last_coaching_sent'] = session_state['windows_processed']
            session_state['coaching_cadence']['coaching_count'] += 1
            state.set(session_uuid, session_state)
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        result = {
            'session_uuid': session_uuid,
            'exercise': window_data.get('exercise', 'unknown'),
            'semantic_analysis': {
                'window_duration_ms': 3000,
                'frames_analyzed': window_data.get('frames_processed', 0),
                'fsm_transitions': [f"{t.get('from', '?')}‚Üí{t.get('to', '?')}" 
                                   for t in window_data.get('fsm_flow', [])],
                'semantic_patterns': window_data.get('semantic_patterns', {}),
                'coaching_categories': window_data.get('coaching_categories', {})
            },
            'requires_coaching': requires_coaching,
            'coaching_metadata': {
                'windows_processed': session_state['windows_processed'],
                'coaching_count': session_state['coaching_cadence']['coaching_count']
            },
            'timestamp': datetime.now().isoformat()
        }
        
        self.logger.info(f"üß† Semantic coaching - Session: {session_uuid}, "
                        f"Patterns: {len(window_data.get('semantic_patterns', {}))}, "
                        f"Coaching: {requires_coaching}")
        
        return result

    def _should_send_semantic_coaching(self, window_result: Dict[str, Any], 
                                     session_state: Dict[str, Any]) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ coaching –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–µ–º–∞–Ω—Ç–∏–∫–∏"""
        
        coaching_categories = window_result.get('coaching_categories', {})
        windows_since_coaching = (session_state['windows_processed'] - 
                                 session_state['coaching_cadence']['last_coaching_sent'])
        
        # –ù–µ–º–µ–¥–ª–µ–Ω–Ω–æ–µ –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–æ –¥–ª—è safety
        if coaching_categories.get('safety_concerns', []):
            return True
        
        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã - –∫–∞–∂–¥—ã–µ 2 –æ–∫–Ω–∞
        if coaching_categories.get('technical_issues', []) and windows_since_coaching >= 2:
            return True
        
        # –ü—Ä–æ–±–ª–µ–º—ã —Ñ–æ—Ä–º—ã - –∫–∞–∂–¥—ã–µ 4 –æ–∫–Ω–∞
        if coaching_categories.get('form_deviations', []) and windows_since_coaching >= 4:
            return True
        
        # –ü—Ä–æ–±–ª–µ–º—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è - –∫–∞–∂–¥—ã–µ 6 –æ–∫–æ–Ω
        if coaching_categories.get('execution_problems', []) and windows_since_coaching >= 6:
            return True
        
        return False

    def _prepare_llm_semantic_input(self, coaching_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        –§–∏–Ω–∞–ª—å–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LLM
        """
        semantic_analysis = coaching_result['semantic_analysis']
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –≤—Ö–æ–¥ –¥–ª—è LLM
        llm_input = {
            'session_id': coaching_result['session_uuid'],
            'exercise_type': coaching_result['exercise'],
            'analysis_timestamp': coaching_result['timestamp'],
            
            # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            'semantic_context': {
                'analysis_window_ms': semantic_analysis['window_duration_ms'],
                'movement_flow': semantic_analysis['fsm_transitions'],
                'error_patterns_detected': len(semantic_analysis['semantic_patterns'])
            },
            
            # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –¥–ª—è LLM
            'coaching_focus_areas': {
                'safety_priorities': self._extract_pattern_details(
                    semantic_analysis.get('coaching_categories', {}).get('safety_concerns', []),
                    semantic_analysis.get('semantic_patterns', {})
                ),
                'technique_improvements': self._extract_pattern_details(
                    semantic_analysis.get('coaching_categories', {}).get('technical_issues', []),
                    semantic_analysis.get('semantic_patterns', {})
                ),
                'form_corrections': self._extract_pattern_details(
                    semantic_analysis.get('coaching_categories', {}).get('form_deviations', []),
                    semantic_analysis.get('semantic_patterns', {})
                ),
                'execution_guidance': self._extract_pattern_details(
                    semantic_analysis.get('coaching_categories', {}).get('execution_problems', []),
                    semantic_analysis.get('semantic_patterns', {})
                )
            },
            
            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è LLM –æ–±—Ä–∞–±–æ—Ç–∫–∏
            'llm_processing_hints': {
                'urgency_level': self._determine_urgency_level(semantic_analysis.get('coaching_categories', {})),
                'coaching_complexity': len(semantic_analysis.get('semantic_patterns', {})),
                'session_maturity': coaching_result.get('coaching_metadata', {}).get('windows_processed', 0),
                'intervention_history': coaching_result.get('coaching_metadata', {}).get('coaching_count', 0)
            }
        }
        
        self.logger.debug(f"ü§ñ LLM input prepared - Focus areas: "
                         f"Safety: {len(llm_input['coaching_focus_areas']['safety_priorities'])}, "
                         f"Technical: {len(llm_input['coaching_focus_areas']['technique_improvements'])}")
        
        return llm_input

    def _extract_pattern_details(self, codes: List[str], patterns: Dict[str, Any]) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª–µ–π –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–ª—è LLM"""
        details = []
        
        for code in codes:
            if code in patterns:
                pattern = patterns[code]
                details.append({
                    'error_code': code,
                    'description': pattern['description'],
                    'frequency': pattern['count'],
                    'coaching_priority': pattern['coaching_priority'],
                    'context_examples': pattern['contexts'][-2:]  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 2 –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                })
        
        return details

    def _determine_urgency_level(self, coaching_categories: Dict[str, List]) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è —Å—Ä–æ—á–Ω–æ—Å—Ç–∏ –¥–ª—è LLM"""
        if coaching_categories.get('safety_concerns', []):
            return 'immediate'
        elif coaching_categories.get('technical_issues', []):
            return 'high'
        elif coaching_categories.get('form_deviations', []):
            return 'medium'
        else:
            return 'low'

    def run(self):
        """–ó–∞–ø—É—Å–∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞"""
        self.logger.info("üöÄ Starting Universal Quality Assessment Processor...")
        
        try:
            sdf = self.create_universal_pipeline()
            self.app.run(sdf)
            
        except KeyboardInterrupt:
            self.logger.info("üõë Universal processor stopped by user")
        except Exception as e:
            self.logger.error(f"üí• Universal processor error: {e}", exc_info=True)
            raise


if __name__ == "__main__":
    processor = UniversalQualityProcessor()
    processor.run()